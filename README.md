# eval-LLM
A small framework to optimize coding and application of evaluation metrics for LLM generated content metrics against groundtruth, the goal is to apply this for different baselines, and add dynamically metrics that I want according to the goal of the evaluation and type of model
